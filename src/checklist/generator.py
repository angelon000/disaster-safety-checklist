"""
ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„± ì—”ì§„
"""
import json
from typing import Dict, List, Any
from datetime import datetime
from pathlib import Path

from checklist.templates import ChecklistTemplates
from collectors.web_researcher import WebResearcher
from collectors.paper_researcher import PaperResearcher
from collectors.tech_researcher import TechResearcher
from collectors.api_researcher import APIResearcher


class ChecklistGenerator:
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ ìë™ ìƒì„±ê¸°"""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.templates = ChecklistTemplates()

        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        self.web_researcher = WebResearcher(
            max_results=self.config.get('max_results_per_source', 10)
        )
        self.paper_researcher = PaperResearcher(
            max_results=self.config.get('max_results_per_source', 10)
        )
        self.tech_researcher = TechResearcher(
            github_token=self.config.get('api_keys', {}).get('github'),
            max_results=self.config.get('max_results_per_source', 10)
        )
        self.api_researcher = APIResearcher(
            public_data_api_key=self.config.get('api_keys', {}).get('public_data')
        )

    def generate(
        self,
        keyword: str,
        facility_type: str,
        check_phase: str,
        focus_area: str = None,
        collect_data: bool = True
    ) -> Dict[str, Any]:
        """
        ë§ì¶¤í˜• ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„±

        Args:
            keyword: ì‹œì„¤/ì‚¬ì—…ì¥ í‚¤ì›Œë“œ
            facility_type: ì‹œì„¤ ìœ í˜•
            check_phase: ì ê²€ ë‹¨ê³„
            focus_area: ê´€ì‹¬ ì˜ì—­
            collect_data: ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰ ì—¬ë¶€

        Returns:
            ìƒì„±ëœ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ì°¸ê³ ìë£Œ
        """
        print(f"\n{'='*60}")
        print(f"ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: {keyword}")
        print(f"ì‹œì„¤ ìœ í˜•: {facility_type}, ì ê²€ ë‹¨ê³„: {check_phase}")
        print(f"{'='*60}\n")

        # 1. í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        template = self.templates.get_template_by_type_and_stage(
            facility_type, check_phase, focus_area
        )

        # 2. ë°ì´í„° ìˆ˜ì§‘ (ì„ íƒ)
        research_data = {}
        if collect_data:
            print("ğŸ“š ë¦¬ì„œì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...\n")
            research_data = self._collect_research_data(keyword)

        # 3. ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ë¦¬ì„œì¹˜ ë§¤í•‘
        enriched_checklist = self._enrich_checklist_with_research(
            template, research_data, keyword
        )

        # 4. ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result = {
            'metadata': {
                'keyword': keyword,
                'facility_type': facility_type,
                'check_phase': check_phase,
                'focus_area': focus_area,
                'generated_at': datetime.now().isoformat(),
                'version': '1.0'
            },
            'checklist': enriched_checklist,
            'research_summary': self._create_research_summary(research_data),
            'recommendations': self._generate_recommendations(research_data)
        }

        print("\nâœ… ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!\n")

        return result

    def _collect_research_data(self, keyword: str) -> Dict[str, Any]:
        """ë¦¬ì„œì¹˜ ë°ì´í„° ìˆ˜ì§‘"""
        data = {
            'web': [],
            'papers': [],
            'tech': [],
            'apis': []
        }

        try:
            # ì›¹ ë¦¬ì„œì¹˜
            print("  ğŸŒ ì›¹ ë¦¬ì„œì¹˜...")
            data['web'] = self.web_researcher.search(keyword)
            print(f"     âœ“ {len(data['web'])} ê±´ ìˆ˜ì§‘")

            # ë…¼ë¬¸ ë¦¬ì„œì¹˜
            print("  ğŸ“„ ë…¼ë¬¸ ë¦¬ì„œì¹˜...")
            data['papers'] = self.paper_researcher.search(keyword)
            print(f"     âœ“ {len(data['papers'])} ê±´ ìˆ˜ì§‘")

            # ê¸°ìˆ  ë¦¬ì„œì¹˜
            print("  ğŸ’» ê¸°ìˆ  íŠ¸ë Œë“œ...")
            data['tech'] = self.tech_researcher.search(keyword)
            print(f"     âœ“ {len(data['tech'])} ê±´ ìˆ˜ì§‘")

            # API ë¦¬ì„œì¹˜
            print("  ğŸ”Œ API ì •ë³´...")
            data['apis'] = self.api_researcher.search(keyword)
            print(f"     âœ“ {len(data['apis'])} ê±´ ìˆ˜ì§‘")

        except Exception as e:
            print(f"  âš ï¸  ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

        return data

    def _enrich_checklist_with_research(
        self,
        template: Dict[str, Any],
        research_data: Dict[str, Any],
        keyword: str
    ) -> Dict[str, Any]:
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ì— ë¦¬ì„œì¹˜ ë°ì´í„° ë§¤í•‘"""
        enriched = {}

        for category_id, category_data in template.items():
            questions = category_data['questions']

            # ê° ì§ˆë¬¸ì— ê´€ë ¨ ë¦¬ì„œì¹˜ ìë£Œ ì—°ê²°
            enriched_questions = []
            for question in questions:
                question_keywords = question.get('research_keywords', [])

                # ê´€ë ¨ ìë£Œ ì°¾ê¸°
                related_resources = self._find_related_resources(
                    question_keywords,
                    research_data,
                    keyword
                )

                enriched_question = {
                    **question,
                    'related_resources': related_resources,
                    'resource_count': len(related_resources),
                    'needs_more_research': len(related_resources) < 3
                }

                enriched_questions.append(enriched_question)

            enriched[category_id] = {
                'info': category_data['info'],
                'questions': enriched_questions,
                'total_resources': sum(q['resource_count'] for q in enriched_questions)
            }

        return enriched

    def _find_related_resources(
        self,
        keywords: List[str],
        research_data: Dict[str, Any],
        main_keyword: str
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë¦¬ì„œì¹˜ ìë£Œ ì°¾ê¸°"""
        resources = []

        # ì›¹ ìë£Œ
        for item in research_data.get('web', []):
            if self._is_relevant(item, keywords, main_keyword):
                resources.append({
                    'type': 'web',
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'source': item.get('source', ''),
                    'summary': item.get('summary', ''),
                    'credibility': item.get('credibility_score', 0.5)
                })

        # ë…¼ë¬¸
        for item in research_data.get('papers', []):
            if self._is_relevant(item, keywords, main_keyword):
                resources.append({
                    'type': 'paper',
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'authors': item.get('authors', []),
                    'year': item.get('year'),
                    'citations': item.get('citations', 0),
                    'source': item.get('source', '')
                })

        # ê¸°ìˆ  ìë£Œ
        for item in research_data.get('tech', []):
            if self._is_relevant(item, keywords, main_keyword):
                resources.append({
                    'type': 'tech',
                    'name': item.get('name', ''),
                    'url': item.get('url', ''),
                    'description': item.get('description', ''),
                    'stars': item.get('stars', 0),
                    'language': item.get('language', ''),
                    'source': item.get('source', '')
                })

        # API ìë£Œ
        for item in research_data.get('apis', []):
            if self._is_relevant(item, keywords, main_keyword):
                resources.append({
                    'type': 'api',
                    'name': item.get('name', ''),
                    'url': item.get('url', ''),
                    'description': item.get('description', ''),
                    'provider': item.get('provider', ''),
                    'usage_policy': item.get('usage_policy', '')
                })

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        resources.sort(key=lambda x: x.get('credibility', 0.5), reverse=True)

        return resources[:5]  # ìƒìœ„ 5ê°œë§Œ

    def _is_relevant(
        self,
        item: Dict[str, Any],
        keywords: List[str],
        main_keyword: str
    ) -> bool:
        """í•­ëª©ì´ í‚¤ì›Œë“œì™€ ê´€ë ¨ìˆëŠ”ì§€ í™•ì¸"""
        # ì œëª©ì´ë‚˜ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        text = (
            item.get('title', '') + ' ' +
            item.get('description', '') + ' ' +
            item.get('summary', '') + ' ' +
            item.get('name', '')
        ).lower()

        # ë©”ì¸ í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
        if main_keyword.lower() in text:
            return True

        # ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        for keyword in keywords:
            if keyword.lower() in text:
                return True

        return False

    def _create_research_summary(self, research_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¦¬ì„œì¹˜ ìš”ì•½ ìƒì„±"""
        return {
            'web_sources': len(research_data.get('web', [])),
            'papers': len(research_data.get('papers', [])),
            'tech_projects': len(research_data.get('tech', [])),
            'apis': len(research_data.get('apis', [])),
            'total_resources': sum([
                len(research_data.get('web', [])),
                len(research_data.get('papers', [])),
                len(research_data.get('tech', [])),
                len(research_data.get('apis', []))
            ]),
            'maturity_analysis': {
                'papers': self.paper_researcher.analyze_maturity(
                    research_data.get('papers', [])
                ),
                'tech': self.tech_researcher.analyze_tech_maturity(
                    research_data.get('tech', [])
                )
            }
        }

    def _generate_recommendations(self, research_data: Dict[str, Any]) -> List[str]:
        """ì¶”ì²œ ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ë…¼ë¬¸ ê¸°ë°˜ ì¶”ì²œ
        papers = research_data.get('papers', [])
        if len(papers) < 5:
            recommendations.append(
                "âš ï¸  ê´€ë ¨ ì—°êµ¬ ìë£Œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì‹  ì•ˆì „ ê¸°ì¤€ì„ ë³„ë„ë¡œ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            )
        elif len(papers) > 20:
            recommendations.append(
                "âœ… ì¶©ë¶„í•œ ì—°êµ¬ ìë£Œê°€ ìˆì–´ ê²€ì¦ëœ ì•ˆì „ ê¸°ì¤€ì„ ì°¸ê³ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        # ê¸°ìˆ  í”„ë¡œì íŠ¸ ê¸°ë°˜ ì¶”ì²œ
        tech = research_data.get('tech', [])
        if tech:
            avg_stars = sum(t.get('stars', 0) for t in tech) / len(tech)
            if avg_stars > 500:
                recommendations.append(
                    "âœ… ê´€ë ¨ ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ë“¤ì´ ìˆì–´ ì¬ë‚œ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•ì´ ìš©ì´í•©ë‹ˆë‹¤."
                )

        # API ê¸°ë°˜ ì¶”ì²œ
        apis = research_data.get('apis', [])
        if apis:
            recommendations.append(
                f"ğŸ’¡ {len(apis)}ê°œì˜ ê³µê³µ APIë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì¬ë‚œ ì •ë³´ë¥¼ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        return recommendations

    def export_to_markdown(self, checklist_data: Dict[str, Any], output_path: str = None):
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ Markdown íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if output_path is None:
            output_dir = Path(self.config.get('output_dir', 'output'))
            output_dir.mkdir(exist_ok=True, parents=True)
            filename = f"checklist_{checklist_data['metadata']['keyword']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            output_path = output_dir / filename

        md_content = self._generate_markdown(checklist_data)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"ğŸ“„ Markdown íŒŒì¼ ìƒì„±: {output_path}")
        return str(output_path)

    def export_to_json(self, checklist_data: Dict[str, Any], output_path: str = None):
        """ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if output_path is None:
            output_dir = Path(self.config.get('output_dir', 'output'))
            output_dir.mkdir(exist_ok=True, parents=True)
            filename = f"checklist_{checklist_data['metadata']['keyword']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            output_path = output_dir / filename

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(checklist_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ JSON íŒŒì¼ ìƒì„±: {output_path}")
        return str(output_path)

    def _generate_markdown(self, data: Dict[str, Any]) -> str:
        """Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        md = []

        # í—¤ë”
        metadata = data['metadata']
        md.append(f"# {metadata['keyword']} - ì¬ë‚œÂ·ì•ˆì „ ì²´í¬ë¦¬ìŠ¤íŠ¸\n")
        md.append(f"**ì‹œì„¤ ìœ í˜•**: {metadata.get('facility_type', metadata.get('content_type', 'N/A'))}\n")
        md.append(f"**ì ê²€ ë‹¨ê³„**: {metadata.get('check_phase', metadata.get('business_stage', 'N/A'))}\n")
        md.append(f"**ìƒì„±ì¼ì‹œ**: {metadata['generated_at']}\n")
        md.append("\n---\n")

        # ë¦¬ì„œì¹˜ ìš”ì•½
        summary = data['research_summary']
        md.append("## ğŸ“Š ë¦¬ì„œì¹˜ ìš”ì•½\n")
        md.append(f"- ì›¹ ìë£Œ: {summary['web_sources']}ê±´\n")
        md.append(f"- ë…¼ë¬¸: {summary['papers']}ê±´\n")
        md.append(f"- ê¸°ìˆ  í”„ë¡œì íŠ¸: {summary['tech_projects']}ê±´\n")
        md.append(f"- API: {summary['apis']}ê±´\n")
        md.append("\n")

        # ì¶”ì²œ ì‚¬í•­
        if data['recommendations']:
            md.append("## ğŸ’¡ ì¶”ì²œ ì‚¬í•­\n")
            for rec in data['recommendations']:
                md.append(f"{rec}\n")
            md.append("\n")

        md.append("---\n\n")

        # ì²´í¬ë¦¬ìŠ¤íŠ¸
        checklist = data['checklist']
        for category_id, category_data in checklist.items():
            info = category_data['info']
            md.append(f"## {info['icon']} {info['name']}\n")
            md.append(f"*{info['description']}*\n\n")

            for i, question in enumerate(category_data['questions'], 1):
                importance_badge = {
                    'high': 'ğŸ”´',
                    'medium': 'ğŸŸ¡',
                    'low': 'ğŸŸ¢'
                }.get(question['importance'], '')

                md.append(f"### {i}. {question['question']} {importance_badge}\n")

                if question['type'] == 'select':
                    md.append("**ì„ íƒì§€:**\n")
                    for option in question.get('options', []):
                        md.append(f"- [ ] {option}\n")
                else:
                    md.append("**ë‹µë³€:**\n\n")
                    md.append("```\n\n```\n")

                # ê´€ë ¨ ìë£Œ
                resources = question.get('related_resources', [])
                if resources:
                    md.append("\n**ì°¸ê³  ìë£Œ:**\n")
                    for res in resources[:3]:  # ìƒìœ„ 3ê°œë§Œ
                        if res['type'] == 'web':
                            md.append(f"- [{res['title']}]({res['url']}) - {res['source']}\n")
                        elif res['type'] == 'paper':
                            md.append(f"- ğŸ“„ [{res['title']}]({res['url']}) ({res.get('year', 'N/A')})\n")
                        elif res['type'] == 'tech':
                            md.append(f"- ğŸ’» [{res['name']}]({res['url']}) - â­ {res.get('stars', 0)}\n")
                        elif res['type'] == 'api':
                            md.append(f"- ğŸ”Œ [{res['name']}]({res['url']}) - {res.get('provider', '')}\n")

                if question.get('needs_more_research'):
                    md.append("\nâš ï¸ *ì¶”ê°€ ë¦¬ì„œì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.*\n")

                md.append("\n")

            md.append("---\n\n")

        return ''.join(md)
